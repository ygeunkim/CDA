# Logistic Regression

```{r, message=FALSE}
library(tidyverse)
# library(knitr)
# library(kableExtra)
# library(formattable)
```

## Horseshoe crab data

```{r, message=FALSE}
(crab <- read_table("data/Crabs.dat"))
```

```{r, eval=FALSE}
crab %>% 
  mutate(
    sat = color_tile("white", "red")(sat),
    y = color_tile("white", "red")(y),
    weight = color_bar("lightblue")(weight),
    width = color_bar("lightgreen")(width),
    color = cell_spec(
      color,
      color = spec_color(color, direction = -1)
    ),
    spine = cell_spec(
      spine,
      color = spec_color(spine)
    )
  ) %>% 
  head() %>% 
  kable(format = "latex", escape = FALSE,
        col.names = c("crab", "Satellites", "y", "Weight(kg)", "carapace width(cm)", "Color", "spine condition")) %>% 
  kable_styling("hover")
```

$$y_i = \begin{cases} 1 & \text{crab}\: i \:\text{has at least one satellite} \\ 0 & \text{crab}\: i \:\text{does not have satellite} \end{cases}$$

> Does the female crab's carapace width is related to this binary response?

Looking at the above data set in the eye, large width can help the crab have satellites. Let's check it out.

```{r crabscat, fig.cap=fig$cap("crabscat", "Number of satellites by width of female crab")}
crab %>% 
  ggplot() +
  aes(width, sat) +
  geom_hex() +
  labs(
    x = "Width",
    y = "Number of Satellite"
  )
```

large variability.

```{r crabdist, fig.cap=fig$cap("crabdist", "Distribution of satellites by width of female crab")}
crab %>% 
  group_by(width_cut = cut(width, 8, ordered_result = TRUE)) %>%
  ggplot() +
  aes(width_cut, sat) +
  geom_boxplot() +
  labs(
    x = "Levels of width",
    y = "Number of Satellite"
  )
```

## Inference for logistic regression

$$logit[\pi(x)] = \alpha + \beta x$$

```{r}
(width_fit <-
  crab %>% 
  select(y, width) %>% 
  glm(y ~ ., data = ., family = binomial())) %>% 
  summary()
```

### Wald test

$$Z = \frac{\hat\beta - \beta_0}{SE} \stackrel{H_0}{\approx} N(0, 1)$$

Equivalently,

$$Z^2 \stackrel{H_0}{\approx} \chi^2_1$$

If multivariate,

$$W = (\boldsymbol{\hat\beta} - \boldsymbol\beta)^T\Big[Cov(\boldsymbol{\hat\beta})\Big]^{-1}(\boldsymbol{\hat\beta} - \boldsymbol\beta) \stackrel{H_0}{\approx} \chi^2_p$$

```{r}
broom::tidy(width_fit) %>% 
  bind_cols(broom::confint_tidy(width_fit)) %>% 
  pander::pander()
```


### Likelihood ratio test

$$G^2 = -2(L_0 - L_1)$$

```{r}
(width_lr <- anova(width_fit, test = "LRT"))
```

### Score test

With dispersion of 1, we have

$$Var(Y_i) = V(\mu_i)$$

and so

$$X^2 = \sum_{i = 1}^n\frac{(y_i - \hat\mu_i)^2}{V(\hat\mu_i)}$$

```{r}
(width_sc <- anova(width_fit, test = "Rao"))
```

In sum,

```{r, echo=FALSE}
wd_global <-
  tribble(
    ~test, ~chi,                 ~df,                ~pval,
    #--------/----------------------/--------------/-----------------------/
    "LRT",   width_lr$Deviance[2], width_lr$Df[2], width_lr$"Pr(>Chi)"[2],
    "Score", width_sc$Rao[2],      width_sc$Df[2], width_sc$"Pr(>Chi)"[2]
  )
pander::pander(wd_global,
               col.names = c("Test", "Chi-Square", "DF", "Pr > ChiSq"))
```

### Confidence interval for logit

$Cov(\boldsymbol{\hat\beta})$ is given as

```{r}
vcov(width_fit)
```

Then

$$Cov(\hat\alpha + \hat\beta x_0) = \left[\begin{array}{cc} 1 & x_0 \end{array}\right] Cov(\boldsymbol{\hat\beta})\left[\begin{array}{c} 1 \\ x_0 \end{array}\right] = Var(\hat\alpha) + x_0^2Var(\hat\beta) + 2x_0Cov(\hat\alpha, \hat\beta)$$

For $x_0 = 26.5$, for instance,

```{r}
x0 <- c(1, 26.5)
t(x0) %*% vcov(width_fit) %*% x0
```

Then we can calculate

$$(\hat\alpha + \hat\beta x_0) + z_{\frac{\alpha}{2}} SE$$

On the other hand, `predict.glm(se.fit = TRUE)` gives above value in `$se.fit` as _standard error_, i.e. squared value.

```{r}
data_frame(width = 26.5) %>% 
  predict(width_fit, newdata = ., type = "link", se.fit = TRUE)
```

Interpolation:

```{r}
(width_logit <-
  crab %>% 
  bind_cols(predict(width_fit, newdata = ., type = "link", se.fit = TRUE) %>% tbl_df()) %>% 
  select(sat, width, fit, se.fit) %>% 
  mutate(
    lower = fit - se.fit * qnorm(.25, lower.tail = FALSE),
    upper = fit + se.fit * qnorm(.25, lower.tail = FALSE)
  ))
```

### Inverse transformation

Noting that

$$\pi(x_0) = \frac{\exp(logit)}{1 + \exp(logit)}$$

```{r}
width_logit %>% 
  transmute(
    sat,
    width,
    lower = exp(lower) / (1 + exp(lower)),
    upper = exp(upper) / (1 + exp(upper))
  )
```

All at once: `type = "response"`

```{r}
predict(width_fit, type = "response", se.fit = TRUE) %>% 
  tbl_df() %>% 
  mutate(
    lower = fit - se.fit * qnorm(.25, lower.tail = FALSE),
    upper = fit + se.fit * qnorm(.25, lower.tail = FALSE)
  )
```

## Goodness of fit

```{r}
anova(width_fit, test = "Chisq")
```

Consider more complex models: _quadtratic model with centered predictor_

```{r}
width_comp <-
  crab %>% 
  mutate(width = width - mean(width)) %>% 
  select(y, width) %>% 
  do(
    null_fit = glm(y ~ 1, data = ., family = binomial()),
    center_fit = glm(y ~ ., data = ., family = binomial()),
    quad_fit = glm(y ~ poly(width, 2), data = ., family = binomial())
  )
```


```{r}
(quad_aov <-
  anova(width_comp$null_fit[[1]],
        width_comp$center_fit[[1]], 
        width_comp$quad_fit[[1]], test = "LRT"))
```

Since quadratic model has $`r quad_aov$"Pr(>Chi)"[3]`$ of p-value, there is no evidence to support the model.

## Hosmer-Lemeshow goodness of fit

```{r}
MKmisc::HLgof.test(fit = fitted(width_fit), obs = crab$y, ngr = 8)
```

```{r}
ResourceSelection::hoslem.test(crab$y, fitted(width_fit), g = 10)
```
